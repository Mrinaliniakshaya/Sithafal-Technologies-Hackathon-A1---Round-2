# Sithafal-Technologies-Hackathon-A1---Round-2

# Retrieval-Augmented Generation (RAG) Pipeline for PDF and Website Interaction

## Overview

This project implements a **Retrieval-Augmented Generation (RAG) pipeline** designed to facilitate interaction with semi-structured data from PDF files and both structured and unstructured data from websites. The goal is to allow users to query this data through natural language and receive context-rich, accurate responses generated by a Large Language Model (LLM). The data is first extracted, chunked, and embedded for efficient retrieval using a vector database, ensuring fast and accurate query responses.

The two main components of this project are:
1. **Chat with PDF**: A system that processes and extracts data from PDF documents, enabling users to query specific information and receive responses based on the extracted data.
2. **Chat with Website**: A system that scrapes and processes data from websites, allowing users to ask questions related to the scraped content.

---

## Features

### **1. Data Ingestion**
- **PDF Data Ingestion**: 
   - Extracts text from PDF files containing semi-structured data.
   - Segments data into logical chunks for better granularity.
   - Converts the chunks into vector embeddings using a pre-trained embedding model (e.g., Sentence-BERT).
   - Stores the embeddings in a vector database (e.g., FAISS, Pinecone) for fast and efficient similarity-based retrieval.

- **Website Data Ingestion**: 
   - Crawls and scrapes content from target websites, including key data fields, metadata, and textual content.
   - Segments the content into logical chunks.
   - Converts the chunks into vector embeddings using a pre-trained embedding model.
   - Stores the embeddings in a vector database with associated metadata for fast retrieval.

### **2. Query Handling**
- Users input natural language queries.
- The queries are converted into vector embeddings using the same embedding model.
- A similarity search is performed in the vector database to retrieve the most relevant chunks of information.
- The retrieved data is passed to the LLM along with a prompt or agentic context to generate a detailed, context-aware response.

### **3. Comparison Queries**
- Handles queries asking for comparisons between different data points (e.g., comparing unemployment rates between various degree types in PDFs or comparing tuition fees across universities from websites).
- Extracts relevant terms or fields for comparison.
- Retrieves the corresponding chunks from the vector database and aggregates data for comparison.
- Presents the results in a structured format, such as a table or bullet points, for easy analysis.

### **4. Response Generation**
- The LLM generates responses by leveraging the context provided from the relevant data chunks retrieved from the vector database.
- The system ensures factuality by directly incorporating the retrieved data into the response, ensuring the information is accurate and aligned with the user's query.

---

## Technologies Used

- **Python 3.8+**
- **Libraries**:
   - **PDF Extraction**: `PyMuPDF` or `pdfplumber` for extracting text and data from PDF files.
   - **Web Scraping**: `BeautifulSoup` and `requests` for crawling and scraping content from websites.
   - **Embedding Generation**: `sentence-transformers` for creating vector embeddings from extracted text or user queries.
   - **Vector Database**: `Pinecone`, `FAISS`, or similar technologies for storing and retrieving vector embeddings.
   - **LLM Integration**: `openai` or other LLM providers for generating detailed responses based on retrieved data.
- **Optional**:
   - **Web Interface**: `Flask` for creating a user-friendly web interface to interact with the system.

---

## Installation

To run the project locally, follow these steps:

1. **Clone the repository**:

   ```bash
   git clone https://github.com/Mrinaliniakshaya/Sithafal-Technologies-Hackathon-A1---Round-2.git
   ```

2. **Install the required dependencies**:

   Navigate to the project directory and install the necessary libraries:

   ```bash
   cd Sithafal-Technologies-Hackathon-A1---Round-2
   pip install -r requirements.txt
   ```

3. **Set up vector database**:
   - You can use **Pinecone** or **FAISS** for vector storage and retrieval. Follow the respective documentation to set up your vector database.
   - If using Pinecone, you will need an API key and the Pinecone client library installed:

     ```bash
     pip install pinecone-client
     ```

4. **Configure environment variables** (if needed):
   - Set up environment variables for API keys (e.g., OpenAI API, Pinecone API, etc.).

---

## Usage

### **1. PDF Interaction**
- **Example Query**: "What is the unemployment rate based on degree type from page 2?"
  - The system will extract the relevant data from the second page of the PDF and provide a response based on the retrieved information.
  
- **Example Query**: "Can you show me the data from page 6 in a tabular format?"
  - The system will extract and present the tabular data from the sixth page of the PDF.

#### Code Example for PDF Extraction:
```python
import fitz  # PyMuPDF

def extract_data_from_pdf(pdf_path, page_number):
    # Open the PDF file
    doc = fitz.open(pdf_path)
    
    # Extract text from the specified page
    page = doc.load_page(page_number)  # page_number is 0-indexed
    text = page.get_text("text")
    
    return text

# Example usage
pdf_path = "example.pdf"
page_number = 1  # Page 2 in the document
text = extract_data_from_pdf(pdf_path, page_number)
print(text)
```

### **2. Website Interaction**
- **Example Query**: "Tell me about the campus life at Stanford University."
  - The system will scrape and retrieve the relevant content about campus life from Stanfordâ€™s website and generate a detailed response.

- **Example Query**: "Compare the tuition fees between the University of Chicago and the University of Washington."
  - The system will retrieve tuition fee data from both websites and present the comparison in a structured format.

#### Code Example for Web Scraping:
```python
import requests
from bs4 import BeautifulSoup

def extract_data_from_website(url):
    # Send a GET request to the website
    response = requests.get(url)
    soup = BeautifulSoup(response.text, "html.parser")
    
    # Extract relevant data from the page (example: title)
    title = soup.find("title").get_text()
    
    return title

# Example usage
url = "https://www.stanford.edu/"
page_title = extract_data_from_website(url)
print(page_title)
```

---

## Example Data

### **Example PDF Data**:
- **PDF Link**: [Unemployment Data PDF](https://www.hunter.cuny.edu/dolciani/pdf_files/workshop-materials/mmc-presentations/tables-charts-and-graphs-with-examples-from.pdf)
  - **Unemployment Information (Page 2)**: Extract data related to unemployment rates based on degree type.
  - **Tabular Data (Page 6)**: Extract and display the tabular data from the sixth page.

### **Example Website Links**:
- [University of Chicago](https://www.uchicago.edu/)
- [University of Washington](https://www.washington.edu/)
- [Stanford University](https://www.stanford.edu/)
- [University of North Dakota](https://und.edu/)


